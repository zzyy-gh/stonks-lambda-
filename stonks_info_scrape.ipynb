{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stonks_info_scrape.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPC+ZTd3MyhqbvPZcLW2vvA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzyy-gh/stonks-lambda-/blob/main/stonks_info_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAhGDoxvKfj3"
      },
      "source": [
        "# scrape stock data daily from finviz\n",
        "# data: tkr, n, sec, ind, ctry, ipo\n",
        "# run on UTC time 0000, 4 hours after NYC: use yesterday's date in utc to get latest market data\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "from multiprocessing import Process, Manager\n",
        "import json\n",
        "import boto3\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "client = boto3.client('dynamodb')\n",
        "\n",
        "# ---------- functions ----------\n",
        "\n",
        "def cleanData(rows, resultsSet, fsList):\n",
        "    \n",
        "    def cleanText(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                cleanItem['PutRequest']['Item'][dataName] = {'S': data}\n",
        "                cleanFsItem[dataName] = data\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    def cleanIPO(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                cleanItem['PutRequest']['Item'][dataName] = {'N': datetime.strptime(item[5], '%m/%d/%Y').strftime('%Y%m%d')}\n",
        "                cleanFsItem[dataName] = datetime.strptime(item[5], '%m/%d/%Y').strftime('%Y%m%d')\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    pageCleanList = []\n",
        "    rawList = []\n",
        "    yesterday = datetime.now() - timedelta(1)\n",
        "    yesterdayStr = yesterday.strftime('%Y%m%d')\n",
        "    \n",
        "    # extract raw data\n",
        "    for row in rows:\n",
        "        td = row.find_all('td')\n",
        "        rawList.append([x.text for x in td])\n",
        "        \n",
        "    # save cleaned data\n",
        "    for item in rawList:\n",
        "        if item[0] != '-' and item[0] not in resultsSet:\n",
        "            cleanItem = {'PutRequest':{'Item':{}}}\n",
        "            cleanFsItem = {}\n",
        "            cleanText('tkr', item[0])\n",
        "            cleanText('n', item[1])\n",
        "            cleanText('sec', item[2])\n",
        "            cleanText('ind', item[3])\n",
        "            cleanText('ctry', item[4])\n",
        "            cleanIPO('ipo', item[5]) \n",
        "            cleanItem['PutRequest']['Item']['date'] = {'N': yesterdayStr}\n",
        "            cleanFsItem['date'] = yesterdayStr\n",
        "    \n",
        "            fsList.append(cleanFsItem)\n",
        "            pageCleanList.append(cleanItem)\n",
        "    \n",
        "    return pageCleanList\n",
        "    \n",
        "def upload(cleanList):\n",
        "    strCleanList = [ [] for _ in range((len(cleanList) - 1) // 25 + 1) ]\n",
        "    unprocList = []\n",
        "    for i, data in enumerate(cleanList):\n",
        "        strCleanList[i // 25].append(data) \n",
        "    for batch in strCleanList:\n",
        "        try: \n",
        "            response = client.batch_write_item(\n",
        "                RequestItems= {\n",
        "                    'ticker_info': batch\n",
        "                }\n",
        "            )\n",
        "            if bool(response['UnprocessedItems']):\n",
        "                print(response)\n",
        "                unprocList.extend(response['UnprocessedItems']['ticker_info'])\n",
        "        except Exception as e:\n",
        "            print(batch[0])\n",
        "            print(e)\n",
        "    if len(unprocList) > 0:\n",
        "        print(str(len(unprocList)) + ' unprocessed items.')\n",
        "        upload(unprocList)\n",
        "    return\n",
        "\n",
        "def endChecker1(nicePage, stopPage):\n",
        "    nextBtn = nicePage.find_all('a', class_='tab-link',string='next')\n",
        "    stopper = nicePage.find_all('a', class_='tab-link', string=stopPage)\n",
        "    \n",
        "    if len(stopper) > 0 or len(nextBtn) == 0:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def endChecker2(nicePage, stopPage):\n",
        "    nextBtn = nicePage.find_all('a', class_='tab-link',string='next')\n",
        "\n",
        "    if len(nextBtn) == 0:\n",
        "        return True\n",
        "    return False\n",
        "    \n",
        "def scrape(startIndex, stopPage, finvizUrl1, finvizUrl2, faker, rowIncrement, finalList, resultsSet, fsList):\n",
        "\n",
        "    # data and setup\n",
        "    row = startIndex\n",
        "    cleanList = []\n",
        "    if stopPage == 'end':\n",
        "        isLastPage = endChecker2\n",
        "    else:\n",
        "        isLastPage = endChecker1\n",
        "    \n",
        "                \n",
        "    # iterate through all the webpages to obtain and clean data\n",
        "    start = time.time()\n",
        "    while True:\n",
        "\n",
        "        # fetch webpage\n",
        "        try:\n",
        "            url = finvizUrl1 + str(row) + finvizUrl2\n",
        "            page = requests.get(url, headers=faker)\n",
        "        except:\n",
        "            print('Failed to retrieve row ' + str(row) + '.')\n",
        "            break\n",
        "\n",
        "        # get page rows\n",
        "        nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "        rows = nicePage.find_all('tr', class_='table-dark-row-cp') + nicePage.find_all('tr', class_='table-light-row-cp')\n",
        "\n",
        "        # if ads pops up, refresh\n",
        "        if len(rows) == 0:\n",
        "            # fetch webpage\n",
        "            try:\n",
        "                print('Retrieving refreshed page of row: ' + str(row) + '.')\n",
        "                page = requests.get(url, headers=faker)\n",
        "            except:\n",
        "                print('Failed to retrieve refreshed page of row: ' + str(row) + '.')\n",
        "                return pageCleanList\n",
        "            nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "            rows = nicePage.find_all('tr', class_='table-light-row-cp') + nicePage.find_all('tr', class_='table-dark-row-cp')\n",
        "        \n",
        "        if len(rows) == 0:\n",
        "            print('Possibly screwed by ads. (page of row: ' + str(row) + ')')\n",
        "        \n",
        "        if len(rows) != 0:\n",
        "            cleanList.extend(cleanData(rows, resultsSet, fsList))\n",
        "\n",
        "        # exit loop if on the last page, else continue to next page\n",
        "        if isLastPage(nicePage, stopPage):\n",
        "            print('Data scraping has ended. The first row number of the last page is ' + str(row) + '.')\n",
        "            break\n",
        "        else:\n",
        "            row += rowIncrement\n",
        "            continue   \n",
        "    print(str(time.time() - start) + ' seconds have elapsed for scraping. (last row ' + str(row) + ')')\n",
        "\n",
        "    # return function if nothing is scraped\n",
        "    if len(cleanList) == 0:\n",
        "        print('No data is scraped. (for startIndex: ' + str(startIndex) + ')')\n",
        "    \n",
        "    finalList.extend(cleanList)\n",
        "    return\n",
        "\n",
        "def uploadFs(prepList, fsHttps):\n",
        "    payload = json.dumps({\n",
        "        'data': prepList\n",
        "    })\n",
        "    headers = {\n",
        "      'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        start = time.time()\n",
        "        response = requests.request(\"POST\", fsHttps, headers=headers, data=payload)\n",
        "        print(str(response.content))\n",
        "        print(str(time.time() - start) + ' seconds have elapsed for firestore upload.')\n",
        "    except:\n",
        "        pass\n",
        "    return\n",
        "    \n",
        "# ---------- main function ----------\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    \n",
        "    # ---------- global variables ----------\n",
        "\n",
        "    marketIsOpen = False\n",
        "    yesterday = datetime.now() - timedelta(1)\n",
        "    date = yesterday.strftime('%Y-%m-%d')\n",
        "    month = yesterday.strftime('%m')\n",
        "    year = yesterday.strftime('%Y')\n",
        "    calendarApi = 'https://sandbox.tradier.com/v1/markets/calendar'\n",
        "    calendarApiToken = 't0XrEyArrcq6EJZAAbP6zbZDl9FA'\n",
        "    fsHttps = 'https://asia-southeast2-stonks-810ca.cloudfunctions.net/addSrapedInfo'\n",
        "    finvizUrl1 = 'https://finviz.com/screener.ashx?v=152&o=ticker&r='\n",
        "    finvizUrl2 = '&c=1,2,3,4,5,70'\n",
        "    faker = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "    rowIncrement = 20\n",
        "    startIndex1 = 1\n",
        "    startIndex2 = 4101\n",
        "    stopPage1 = '205'\n",
        "    stopPage2 = 'end'\n",
        "    table_name = 'ticker_meta'\n",
        "    results = []\n",
        "    resultsSet = set()\n",
        "    last_evaluated_key = None\n",
        "\n",
        "    # ---------- check if market is open, else return ----------\n",
        "\n",
        "    calendar = requests.get(calendarApi, \n",
        "                     headers={'Authorization': calendarApiToken, 'Accept': 'application/json'},\n",
        "                     params={'month': month, 'year': year})\n",
        "\n",
        "    dayListInThisMonth = calendar.json()['calendar']['days']['day']\n",
        "    try:\n",
        "        for day in dayListInThisMonth:\n",
        "            if day['date'] == date:\n",
        "                if day['status'] == 'open':\n",
        "                    marketIsOpen = True\n",
        "                    break\n",
        "        if marketIsOpen:\n",
        "            print('Market is open.')\n",
        "        else:\n",
        "            print('Market is closed.')        \n",
        "    except:\n",
        "        print('Failed to check market\\'s status.')\n",
        "\n",
        "    if not marketIsOpen:\n",
        "        return\n",
        "    \n",
        "# ---------- check existing tkr ----------\n",
        "    \n",
        "    # find tkr\n",
        "    while True:\n",
        "        try:\n",
        "            if last_evaluated_key:\n",
        "                response = client.scan(\n",
        "                    TableName=table_name,\n",
        "                    ExclusiveStartKey=last_evaluated_key,\n",
        "                    ExpressionAttributeNames={\n",
        "                        '#t': 'tkr',\n",
        "                    },\n",
        "                    ProjectionExpression='#t',\n",
        "                )\n",
        "            else: \n",
        "                response = client.scan(\n",
        "                    TableName=table_name,\n",
        "                    ExpressionAttributeNames={\n",
        "                        '#t': 'tkr',\n",
        "                    },\n",
        "                    ProjectionExpression='#t',\n",
        "                    )\n",
        "            last_evaluated_key = response.get('LastEvaluatedKey')\n",
        "            \n",
        "            results.extend(response['Items'])\n",
        "            \n",
        "            if not last_evaluated_key:\n",
        "                break\n",
        "        except:\n",
        "            print('aww shit')\n",
        "    for i in results:\n",
        "        resultsSet.add(i['tkr']['S'])\n",
        "    print(str(len(resultsSet)) + ' tickers in db.')\n",
        "    \n",
        "\n",
        "    # ---------- scrape and upload data from finviz ----------\n",
        "    \n",
        "    with Manager() as manager:\n",
        "        # creating processes\n",
        "        start = time.time()\n",
        "        finalList = manager.list()\n",
        "        fsList = manager.list()\n",
        "        p1 = Process(target=scrape, args=(startIndex1, stopPage1, finvizUrl1, finvizUrl2, faker, rowIncrement, finalList, resultsSet, fsList, ))\n",
        "        p2 = Process(target=scrape, args=(startIndex2, stopPage2, finvizUrl1, finvizUrl2, faker, rowIncrement, finalList, resultsSet, fsList,))\n",
        "\n",
        "        # starting process 1\n",
        "        p1.start()\n",
        "        # starting process 2\n",
        "        p2.start()\n",
        "\n",
        "        # wait until process 1 is finished\n",
        "        p1.join()\n",
        "        # wait until process 2 is finished\n",
        "        p2.join()\n",
        "        print(str(time.time() - start) + ' seconds have elapsed for scraping in total.')\n",
        "    \n",
        "        # upload to dynamodb and firestore\n",
        "        print(str(len(finalList)) + ' rows to be uploaded.')\n",
        "        if len(finalList) == 0:\n",
        "            return\n",
        "        start = time.time()\n",
        "        prepFinalList = finalList._getvalue()\n",
        "        p1 = Process(target=upload, args=(prepFinalList, ))\n",
        "        prepList = fsList._getvalue()\n",
        "        p2 = Process(target=uploadFs, args=(prepList, fsHttps, ))\n",
        "        \n",
        "        # starting process 1\n",
        "        p1.start()\n",
        "        # starting process 2\n",
        "        p2.start()\n",
        "\n",
        "        # wait until process 1 is finished\n",
        "        p1.join()\n",
        "        # wait until process 2 is finished\n",
        "        p2.join()\n",
        "        print(str(time.time() - start) + ' seconds have elapsed for both uploads.')\n",
        "\n",
        "    return \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}