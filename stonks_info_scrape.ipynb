{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stonks_info_scrape.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8HiVJeiBRQrFBygXFYwLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzyy-gh/stonks-lambda-/blob/main/stonks_info_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usovU50ZLHVs"
      },
      "source": [
        "# scrape stock data daily from finviz\n",
        "# data: tkr, n, sec, ind, ctry, ipo\n",
        "# run on UTC time 0000, 4 hours after NYC: use yesterday's date in utc to get latest market data\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import multiprocessing\n",
        "import json\n",
        "import boto3\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "client = boto3.client('dynamodb')\n",
        "\n",
        "# ---------- functions ----------\n",
        "\n",
        "def cleanData(nicePage, url, faker):\n",
        "    \n",
        "    def cleanText(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                cleanItem['PutRequest']['Item'][dataName] = {'S': data}\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    def cleanIPO(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                cleanItem['PutRequest']['Item'][dataName] = {'N': datetime.strptime(item[5], '%m/%d/%Y').strftime('%Y%m%d')}\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    rows = nicePage.find_all('tr', class_='table-light-row-cp') + nicePage.find_all('tr', class_='table-dark-row-cp')\n",
        "    pageCleanList = []\n",
        "    rawList = []\n",
        "    \n",
        "    # if ads pops up, refresh\n",
        "    if len(rows) == 0:\n",
        "        # fetch webpage\n",
        "        try:\n",
        "            print('Retrieving refreshed page.')\n",
        "            page = requests.get(url, headers=faker)\n",
        "        except:\n",
        "            print('Failed to retrieve refreshed page.')\n",
        "            return pageCleanList\n",
        "        nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "        rows = nicePage.find_all('tr', class_='table-light-row-cp') + nicePage.find_all('tr', class_='table-dark-row-cp')\n",
        "    \n",
        "    # extract raw data\n",
        "    for row in rows:\n",
        "        td = row.find_all('td')\n",
        "        rawList.append([x.text for x in td])\n",
        "        \n",
        "    # save cleaned data\n",
        "    for item in rawList:\n",
        "        try:\n",
        "            date = datetime.strptime(item[5], \"%m/%d/%Y\")\n",
        "            yesterday = datetime.now() - timedelta(1)\n",
        "            if date.date() < yesterday.date():\n",
        "                print('No new IPO.')\n",
        "                return pageCleanList\n",
        "            else:\n",
        "                print('New stock!')\n",
        "                pass\n",
        "        except:\n",
        "            pass\n",
        "        cleanItem = {'PutRequest':{'Item':{}}}\n",
        "        cleanText('tkr', item[0])\n",
        "        cleanText('n', item[1])\n",
        "        cleanText('sec', item[2])\n",
        "        cleanText('ind', item[3])\n",
        "        cleanText('ctry', item[4])\n",
        "        cleanIPO('ipo', item[5]) \n",
        "        cleanItem['PutRequest']['Item']['date'] = {'N': yesterday.strftime('%Y%m%d')}\n",
        "\n",
        "        pageCleanList.append(cleanItem)\n",
        "        \n",
        "    \n",
        "    return pageCleanList\n",
        "    \n",
        "def upload(cleanList):\n",
        "    strCleanList = [ [] for _ in range((len(cleanList) - 1) // 25 + 1) ]\n",
        "    for i, data in enumerate(cleanList):\n",
        "        strCleanList[i // 25].append(data) \n",
        "    for batch in strCleanList:\n",
        "        try: \n",
        "            response = client.batch_write_item(\n",
        "                RequestItems= {\n",
        "                    'ticker_info': batch\n",
        "                }\n",
        "            )\n",
        "        except:\n",
        "            print('Whoops!')\n",
        "    return\n",
        "\n",
        "def scrape_n_store(finvizUrl1, finvizUrl2, faker, rowIncrement):\n",
        "\n",
        "    # data\n",
        "    row = 1\n",
        "    cleanList = []\n",
        "\n",
        "    # iterate through all the webpages to obtain and clean data\n",
        "    start = time.time()\n",
        "    while True:\n",
        "\n",
        "        # fetch webpage\n",
        "        try:\n",
        "            url = finvizUrl1 + str(row) + finvizUrl2\n",
        "            page = requests.get(url, headers=faker)\n",
        "        except:\n",
        "            print('Failed to retrieve row ' + str(row) + '. (1)')\n",
        "            break\n",
        "\n",
        "        # get clean data\n",
        "        nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "        tempCleanList = cleanData(nicePage, url, faker)\n",
        "        if len(tempCleanList) == 0:\n",
        "            print('No new stocks.')\n",
        "            return\n",
        "        else:\n",
        "            cleanList.extend(tempCleanList)\n",
        "\n",
        "\n",
        "        # exit loop if on the last page, else continue to next page\n",
        "        nextBtn = nicePage.find_all('a', class_='tab-link',string='next')\n",
        "        if len(nextBtn) == 0:\n",
        "            print('Data scraping has ended. The first row number of the last page is ' + str(row) + '. (1)')\n",
        "            break\n",
        "        else:\n",
        "            row += rowIncrement\n",
        "            continue   \n",
        "    print(str(time.time() - start) + ' seconds have elapsed for scraping. (1)')\n",
        "\n",
        "    # return function if nothing is scraped\n",
        "    if len(cleanList) == 0:\n",
        "        print('No data is scraped. (1)')\n",
        "        return\n",
        "    \n",
        "    # upload to dynamodb\n",
        "    print(str(len(cleanList)) + ' rows to be added to dynamodb. (1)')\n",
        "    start = time.time()\n",
        "    upload(cleanList)\n",
        "    print(str(time.time() - start) + ' seconds have elapsed for uploading. (1)')\n",
        "    \n",
        "# ---------- main function ----------\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    \n",
        "    # ---------- global variables ----------\n",
        "\n",
        "    marketIsOpen = False\n",
        "    yesterday = datetime.now() - timedelta(1)\n",
        "    date = yesterday.strftime('%Y-%m-%d')\n",
        "    month = yesterday.strftime('%m')\n",
        "    year = yesterday.strftime('%Y')\n",
        "    calendarApi = 'https://sandbox.tradier.com/v1/markets/calendar'\n",
        "    calendarApiToken = 't0XrEyArrcq6EJZAAbP6zbZDl9FA'\n",
        "    finvizUrl1 = 'https://finviz.com/screener.ashx?v=152&o=ticker&r='\n",
        "    finvizUrl2 = '&c=1,2,3,4,5,70'\n",
        "    faker = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "    rowIncrement = 20\n",
        "\n",
        "    # ---------- check if market is open ----------\n",
        "\n",
        "    calendar = requests.get(calendarApi, \n",
        "                     headers={'Authorization': calendarApiToken, 'Accept': 'application/json'},\n",
        "                     params={'month': month, 'year': year})\n",
        "\n",
        "    dayListInThisMonth = calendar.json()['calendar']['days']['day']\n",
        "    try:\n",
        "        for day in dayListInThisMonth:\n",
        "            if day['date'] == date:\n",
        "                if day['status'] == 'open':\n",
        "                    marketIsOpen = True\n",
        "                    break\n",
        "        if marketIsOpen:\n",
        "            print('Market is open.')\n",
        "        else:\n",
        "            print('Market is closed.')        \n",
        "    except:\n",
        "        print('Failed to check market\\'s status.')\n",
        "\n",
        "    # ---------- return function if market is closed ----------\n",
        "\n",
        "    if not marketIsOpen:\n",
        "        return\n",
        "\n",
        "    # ---------- scrape data from finviz ----------\n",
        "    \n",
        "    scrape_n_store(finvizUrl1, finvizUrl2, faker, rowIncrement)\n",
        "    \n",
        "    return \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}