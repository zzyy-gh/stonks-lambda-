{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stonks_info_scrape.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOhT12RM9ID+1jO8YTlcGNK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzyy-gh/stonks-lambda-/blob/main/stonks_info_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAhGDoxvKfj3"
      },
      "source": [
        "# scrape stock data daily from finviz\n",
        "# data: tkr, n, sec, ind, ctry, ipo\n",
        "# run on UTC time 0000, 4 hours after NYC: use yesterday's date in utc to get latest market data\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "from multiprocessing import Process, Manager\n",
        "import json\n",
        "import boto3\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "client = boto3.client('dynamodb')\n",
        "\n",
        "# ---------- functions ----------\n",
        "\n",
        "def cleanData(rows):\n",
        "    \n",
        "    def cleanText(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                cleanItem['PutRequest']['Item'][dataName] = {'S': data}\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    def cleanIPO(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                cleanItem['PutRequest']['Item'][dataName] = {'N': datetime.strptime(item[5], '%m/%d/%Y').strftime('%Y%m%d')}\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    pageCleanList = []\n",
        "    rawList = []\n",
        "    yesterday = datetime.now() - timedelta(1)\n",
        "    yesterdayStr = yesterday.strftime('%Y%m%d')\n",
        "    \n",
        "    # extract raw data\n",
        "    for row in rows:\n",
        "        td = row.find_all('td')\n",
        "        rawList.append([x.text for x in td])\n",
        "        \n",
        "    # save cleaned data\n",
        "    for item in rawList:\n",
        "        cleanItem = {'PutRequest':{'Item':{}}}\n",
        "        cleanText('tkr', item[0])\n",
        "        cleanText('n', item[1])\n",
        "        cleanText('sec', item[2])\n",
        "        cleanText('ind', item[3])\n",
        "        cleanText('ctry', item[4])\n",
        "        cleanIPO('ipo', item[5]) \n",
        "        cleanItem['PutRequest']['Item']['date'] = {'N': yesterdayStr}\n",
        "\n",
        "\n",
        "        pageCleanList.append(cleanItem)\n",
        "    \n",
        "    return pageCleanList\n",
        "    \n",
        "def upload(cleanList):\n",
        "    strCleanList = [ [] for _ in range((len(cleanList) - 1) // 25 + 1) ]\n",
        "    unprocList = []\n",
        "    for i, data in enumerate(cleanList):\n",
        "        strCleanList[i // 25].append(data) \n",
        "    for batch in strCleanList:\n",
        "        try: \n",
        "            response = client.batch_write_item(\n",
        "                RequestItems= {\n",
        "                    'ticker_info': batch\n",
        "                }\n",
        "            )\n",
        "            if bool(response['UnprocessedItems']):\n",
        "                print(response)\n",
        "                unprocList.extend(response['UnprocessedItems']['ticker_info'])\n",
        "        except Exception as e:\n",
        "            print(batch[0])\n",
        "            print(e)\n",
        "    if len(unprocList) > 0:\n",
        "        print(str(len(unprocList)) + ' unprocessed items.')\n",
        "        upload(unprocList)\n",
        "    return\n",
        "\n",
        "def endChecker1(nicePage, stopPage):\n",
        "    nextBtn = nicePage.find_all('a', class_='tab-link',string='next')\n",
        "    stopper = nicePage.find_all('a', class_='tab-link', string=stopPage)\n",
        "    \n",
        "    if len(stopper) > 0 or len(nextBtn) == 0:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def endChecker2(nicePage, stopPage):\n",
        "    nextBtn = nicePage.find_all('a', class_='tab-link',string='next')\n",
        "\n",
        "    if len(nextBtn) == 0:\n",
        "        return True\n",
        "    return False\n",
        "    \n",
        "def scrape(startIndex, stopPage, finvizUrl1, finvizUrl2, faker, rowIncrement, finalList):\n",
        "\n",
        "    # data and setup\n",
        "    row = startIndex\n",
        "    cleanList = []\n",
        "    if stopPage == 'end':\n",
        "        isLastPage = endChecker2\n",
        "    else:\n",
        "        isLastPage = endChecker1\n",
        "    \n",
        "                \n",
        "    # iterate through all the webpages to obtain and clean data\n",
        "    start = time.time()\n",
        "    while True:\n",
        "\n",
        "        # fetch webpage\n",
        "        try:\n",
        "            url = finvizUrl1 + str(row) + finvizUrl2\n",
        "            page = requests.get(url, headers=faker)\n",
        "        except:\n",
        "            print('Failed to retrieve row ' + str(row) + '.')\n",
        "            break\n",
        "\n",
        "        # get page rows\n",
        "        nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "        rows = nicePage.find_all('tr', class_='table-dark-row-cp') + nicePage.find_all('tr', class_='table-light-row-cp')\n",
        "\n",
        "        # if ads pops up, refresh\n",
        "        if len(rows) == 0:\n",
        "            # fetch webpage\n",
        "            try:\n",
        "                print('Retrieving refreshed page. (for startIndex: ' + str(startIndex) + ')')\n",
        "                page = requests.get(url, headers=faker)\n",
        "            except:\n",
        "                print('Failed to retrieve refreshed page (for startIndex: ' + str(startIndex) + ')')\n",
        "                return pageCleanList\n",
        "            nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "            rows = nicePage.find_all('tr', class_='table-light-row-cp') + nicePage.find_all('tr', class_='table-dark-row-cp')\n",
        "        \n",
        "        if len(rows) != 0:\n",
        "            cleanList.extend(cleanData(rows))\n",
        "\n",
        "        # exit loop if on the last page, else continue to next page\n",
        "        if isLastPage(nicePage, stopPage):\n",
        "            print('Data scraping has ended. The first row number of the last page is ' + str(row) + '.')\n",
        "            break\n",
        "        else:\n",
        "            row += rowIncrement\n",
        "            continue   \n",
        "    print(str(time.time() - start) + ' seconds have elapsed for scraping. (last row ' + str(row) + ')')\n",
        "\n",
        "    # return function if nothing is scraped\n",
        "    if len(cleanList) == 0:\n",
        "        print('No data is scraped. (for startIndex: ' + str(startIndex) + ')')\n",
        "    \n",
        "    finalList.extend(cleanList)\n",
        "    return\n",
        "    \n",
        "# ---------- main function ----------\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    \n",
        "    # ---------- global variables ----------\n",
        "\n",
        "    marketIsOpen = False\n",
        "    yesterday = datetime.now() - timedelta(1)\n",
        "    date = yesterday.strftime('%Y-%m-%d')\n",
        "    month = yesterday.strftime('%m')\n",
        "    year = yesterday.strftime('%Y')\n",
        "    calendarApi = 'https://sandbox.tradier.com/v1/markets/calendar'\n",
        "    calendarApiToken = 't0XrEyArrcq6EJZAAbP6zbZDl9FA'\n",
        "    finvizUrl1 = 'https://finviz.com/screener.ashx?v=152&o=ticker&r='\n",
        "    finvizUrl2 = '&c=1,2,3,4,5,70'\n",
        "    faker = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "    rowIncrement = 20\n",
        "    startIndex1 = 1\n",
        "    startIndex2 = 4101\n",
        "    stopPage1 = '205'\n",
        "    stopPage2 = 'end'\n",
        "\n",
        "    # ---------- check if market is open, else return ----------\n",
        "\n",
        "    calendar = requests.get(calendarApi, \n",
        "                     headers={'Authorization': calendarApiToken, 'Accept': 'application/json'},\n",
        "                     params={'month': month, 'year': year})\n",
        "\n",
        "    dayListInThisMonth = calendar.json()['calendar']['days']['day']\n",
        "    try:\n",
        "        for day in dayListInThisMonth:\n",
        "            if day['date'] == date:\n",
        "                if day['status'] == 'open':\n",
        "                    marketIsOpen = True\n",
        "                    break\n",
        "        if marketIsOpen:\n",
        "            print('Market is open.')\n",
        "        else:\n",
        "            print('Market is closed.')        \n",
        "    except:\n",
        "        print('Failed to check market\\'s status.')\n",
        "\n",
        "    if not marketIsOpen:\n",
        "        return\n",
        "\n",
        "    # ---------- scrape and upload data from finviz ----------\n",
        "    \n",
        "    with Manager() as manager:\n",
        "        # creating processes\n",
        "        start = time.time()\n",
        "        finalList = manager.list()\n",
        "        p1 = Process(target=scrape, args=(startIndex1, stopPage1, finvizUrl1, finvizUrl2, faker, rowIncrement, finalList, ))\n",
        "        p2 = Process(target=scrape, args=(startIndex2, stopPage2, finvizUrl1, finvizUrl2, faker, rowIncrement, finalList, ))\n",
        "\n",
        "        # starting process 1\n",
        "        p1.start()\n",
        "        # starting process 2\n",
        "        p2.start()\n",
        "\n",
        "        # wait until process 1 is finished\n",
        "        p1.join()\n",
        "        # wait until process 2 is finished\n",
        "        p2.join()\n",
        "        print(str(time.time() - start) + ' seconds have elapsed for scraping in total.')\n",
        "    \n",
        "        # upload to dynamodb\n",
        "        print(str(len(finalList)) + ' rows to be added to dynamodb.')\n",
        "        start = time.time()\n",
        "        upload(finalList)\n",
        "        print(str(time.time() - start) + ' seconds have elapsed for uploading.')\n",
        "\n",
        "    return \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}