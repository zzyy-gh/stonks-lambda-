{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stonks_scrape(checker).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPeU3V9I6vwo3Kf8d9uY7AA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzyy-gh/stonks_scrape/blob/main/stonks_scrape(checker).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiMs-drGBO9t"
      },
      "source": [
        "# scrape stock data daily from finviz\n",
        "# data: tkr, pb, sgqq, s, f, (sf), si, sti, fsh, de, opm, prfm, weekc, monc, quartc, halfc, yearc, weekv, monv, p, dayc, v\n",
        "# run on UTC time 0000, 4 hours after NYC: use yesterday's date in utc to get latest market data\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import multiprocessing\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ---------- functions ----------\n",
        "\n",
        "def cleanData(rows):\n",
        "    \n",
        "    def cleanText(dataName, data):\n",
        "        try:\n",
        "            cleanItem[dataName] = data\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "\n",
        "    def cleanSimpleFloat(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                cleanItem[dataName] = data\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    def cleanCommaInt(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                cleanItem[dataName] = data.replace(',', '')\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    def cleanPercFloat(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                cleanItem[dataName] = data[:-1]\n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    def cleanMBFloat(dataName, data):\n",
        "        try:\n",
        "            if data != '-':\n",
        "                if data[-1] == 'M':\n",
        "                    cleanItem[dataName] = str(float(data[:-1]) * 1000000)\n",
        "                elif data[-1] == 'B':\n",
        "                    cleanItem[dataName] = str(float(data[:-1]) * 1000000000)              \n",
        "        except:\n",
        "            pass\n",
        "        return\n",
        "    \n",
        "    date = (datetime.now() - timedelta(1)).strftime('%Y%m%d')\n",
        "    pageCleanList = []\n",
        "    rawList = []\n",
        "    \n",
        "    # extract raw data\n",
        "    for row in rows:\n",
        "        td = row.find_all('td')\n",
        "        rawList.append([x.text for x in td])\n",
        "        \n",
        "    # save cleaned data\n",
        "    for item in rawList:\n",
        "        cleanItem = {}\n",
        "        cleanText('tkr', item[0])\n",
        "        cleanSimpleFloat('pb', item[1])\n",
        "        cleanPercFloat('sgqq', item[2])\n",
        "        cleanMBFloat('s', item[3])\n",
        "        cleanMBFloat('f', item[4])\n",
        "        cleanPercFloat('si', item[5])\n",
        "        cleanPercFloat('sti', item[6])\n",
        "        cleanPercFloat('fsh', item[7])\n",
        "        cleanSimpleFloat('de', item[8])\n",
        "        cleanPercFloat('opm', item[9])\n",
        "        cleanPercFloat('prfm', item[10])\n",
        "        cleanPercFloat('weekc', item[11])\n",
        "        cleanPercFloat('monc', item[12])\n",
        "        cleanPercFloat('quartc', item[13])\n",
        "        cleanPercFloat('halfc', item[14])\n",
        "        cleanPercFloat('yearc', item[15])\n",
        "        cleanPercFloat('weekv', item[16])\n",
        "        cleanPercFloat('monv', item[17])\n",
        "        cleanSimpleFloat('p', item[18])\n",
        "        cleanPercFloat('dayc', item[19])\n",
        "        cleanCommaInt('v', item[20])\n",
        "        try:\n",
        "            if cleanItem['s'] and cleanItem['f']:\n",
        "                cleanItem['sf'] = str(round(item[3]/item[4], 2))\n",
        "        except:\n",
        "            pass\n",
        "        cleanItem['uuid'] = date + item[0]\n",
        "        cleanItem['date'] = date\n",
        "\n",
        "        pageCleanList.append(cleanItem)\n",
        "    \n",
        "    return pageCleanList\n",
        "\n",
        "def scrape_n_store1(stopPage, finvizUrl1, finvizUrl2, faker, rowIncrement):\n",
        "\n",
        "    # data\n",
        "    row = 1\n",
        "    cleanList = []\n",
        "\n",
        "    # iterate through all the webpages to obtain and clean data\n",
        "    start = time.time()\n",
        "    while True:\n",
        "\n",
        "        # fetch webpage\n",
        "        try:\n",
        "            url = finvizUrl1 + str(row) + finvizUrl2\n",
        "            page = requests.get(url, headers=faker)\n",
        "        except:\n",
        "            print('Failed to retrieve row ' + str(row) + '. (1)')\n",
        "            break\n",
        "\n",
        "        # get clean data\n",
        "        nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "        rows = nicePage.find_all('tr', class_='table-dark-row-cp') + nicePage.find_all('tr', class_='table-light-row-cp')\n",
        "        # if ads pops up, refresh\n",
        "        if len(rows) == 0:\n",
        "            # fetch webpage\n",
        "            try:\n",
        "                print('Retrieving refreshed page.')\n",
        "                page = requests.get(url, headers=faker)\n",
        "            except:\n",
        "                print('Failed to retrieve refreshed page.')\n",
        "                return\n",
        "            nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "            rows = nicePage.find_all('tr', class_='table-light-row-cp') + nicePage.find_all('tr', class_='table-dark-row-cp')\n",
        "        cleanList.extend(cleanData(rows))\n",
        "\n",
        "        # exit loop if on the last page, else continue to next page\n",
        "        stopper = nicePage.find_all('a', class_='tab-link', string=stopPage)\n",
        "        nextBtn = nicePage.find_all('a', class_='tab-link',string='next')\n",
        "        if len(stopper) > 0 or len(nextBtn) == 0:\n",
        "            print('Data scraping has ended. The first row number of the last page is ' + str(row) + '. (1)')\n",
        "            break\n",
        "        else:\n",
        "            row += rowIncrement\n",
        "            continue   \n",
        "    print(str(time.time() - start) + ' seconds have elapsed for scraping. (1)')\n",
        "\n",
        "    # return function if nothing is scraped\n",
        "    if len(cleanList) == 0:\n",
        "        print('No data is scraped. (1)')\n",
        "        return\n",
        "    \n",
        "    print(len(cleanList))\n",
        "    print(cleanList)\n",
        "    return\n",
        "    \n",
        "def scrape_n_store2(startIndex, finvizUrl1, finvizUrl2, faker, rowIncrement):\n",
        "\n",
        "    # data\n",
        "    row = startIndex\n",
        "    cleanList = []\n",
        "                \n",
        "    # iterate through all the webpages to obtain and clean data\n",
        "    start = time.time()\n",
        "    while True:\n",
        "\n",
        "        # fetch webpage\n",
        "        try:\n",
        "            url = finvizUrl1 + str(row) + finvizUrl2\n",
        "            page = requests.get(url, headers=faker)\n",
        "        except:\n",
        "            print('Failed to retrieve row ' + str(row) + '. (2)')\n",
        "            break\n",
        "\n",
        "        # get clean data\n",
        "        nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "        rows = nicePage.find_all('tr', class_='table-dark-row-cp') + nicePage.find_all('tr', class_='table-light-row-cp')\n",
        "        # if ads pops up, refresh\n",
        "        if len(rows) == 0:\n",
        "            # fetch webpage\n",
        "            try:\n",
        "                print('Retrieving refreshed page.')\n",
        "                page = requests.get(url, headers=faker)\n",
        "            except:\n",
        "                print('Failed to retrieve refreshed page.')\n",
        "                return\n",
        "            nicePage = BeautifulSoup(page.content, 'html.parser')\n",
        "            rows = nicePage.find_all('tr', class_='table-light-row-cp') + nicePage.find_all('tr', class_='table-dark-row-cp')\n",
        "        cleanList.extend(cleanData(rows))\n",
        "\n",
        "        # exit loop if on the last page, else continue to next page\n",
        "        nextBtn = nicePage.find_all('a', class_='tab-link',string='next')\n",
        "        if len(nextBtn) == 0:\n",
        "            print('Data scraping has ended. The first row number of the last page is ' + str(row) + '. (2)')\n",
        "            break\n",
        "        else:\n",
        "            row += rowIncrement\n",
        "            continue   \n",
        "    print(str(time.time() - start) + ' seconds have elapsed for scraping. (2)')\n",
        "\n",
        "    # return function if nothing is scraped\n",
        "    if len(cleanList) == 0:\n",
        "        print('No data is scraped. (2)')\n",
        "        return\n",
        "    \n",
        "    print(len(cleanList))\n",
        "    print(cleanList)\n",
        "    return\n",
        "    \n",
        "# ---------- main function ----------\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    \n",
        "    # ---------- global variables ----------\n",
        "\n",
        "    marketIsOpen = False\n",
        "    yesterday = datetime.now() - timedelta(1)\n",
        "    date = yesterday.strftime('%Y-%m-%d')\n",
        "    month = yesterday.strftime('%m')\n",
        "    year = yesterday.strftime('%Y')\n",
        "    calendarApi = 'https://sandbox.tradier.com/v1/markets/calendar'\n",
        "    calendarApiToken = 't0XrEyArrcq6EJZAAbP6zbZDl9FA'\n",
        "    finvizUrl1 = 'https://finviz.com/screener.ashx?v=152&o=ticker&r='\n",
        "    finvizUrl2 = '&c=1,11,23,24,25,26,28,30,38,40,41,42,43,44,45,46,50,51,65,66,67'\n",
        "    faker = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "    rowIncrement = 20\n",
        "    startIndex = 4101\n",
        "    stopPage = '205'\n",
        "\n",
        "    # ---------- check if market is open ----------\n",
        "\n",
        "    calendar = requests.get(calendarApi, \n",
        "                     headers={'Authorization': calendarApiToken, 'Accept': 'application/json'},\n",
        "                     params={'month': month, 'year': year})\n",
        "\n",
        "    dayListInThisMonth = calendar.json()['calendar']['days']['day']\n",
        "    try:\n",
        "        for day in dayListInThisMonth:\n",
        "            if day['date'] == date:\n",
        "                if day['status'] == 'open':\n",
        "                    marketIsOpen = True\n",
        "                    break\n",
        "        if marketIsOpen:\n",
        "            print('Market is open.')\n",
        "        else:\n",
        "            print('Market is closed.')        \n",
        "    except:\n",
        "        print('Failed to check market\\'s status.')\n",
        "\n",
        "    # ---------- return function if market is closed ----------\n",
        "\n",
        "    if not marketIsOpen:\n",
        "        return\n",
        "\n",
        "    # ---------- scrape data from finviz ----------\n",
        "\n",
        "    scrape_n_store1(stopPage, finvizUrl1, finvizUrl2, faker, rowIncrement)\n",
        "    scrape_n_store2(startIndex, finvizUrl1, finvizUrl2, faker, rowIncrement)\n",
        "\n",
        "    return \n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C9JPUbLl5yl",
        "outputId": "dcc128d2-32cc-4618-88f8-fd419328dd8a"
      },
      "source": [
        "lambda_handler(1, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Market is open.\n"
          ]
        }
      ]
    }
  ]
}